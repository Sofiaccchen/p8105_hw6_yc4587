---
title: "homework 6"
output: html_document
date: "2024-11-25"
---
Problem 1
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rnoaa)
library(broom)
```


```{r load_data}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```
)

```{r bootstrap_function}
bootstrap_fn = function(df) {
  sample_df = sample_n(df, size = nrow(df), replace = TRUE)
  
  fit = lm(tmax ~ tmin, data = sample_df)

  rsquared = summary(fit)$r.squared
  beta0 = coef(fit)[1]
  beta1 = coef(fit)[2]
  log_product = log(beta0 * beta1)
  
  return(tibble(
    r_squared = rsquared,
    log_beta_product = log_product
  ))
}

set.seed(123)  
bootstrap_results = 
  tibble(strap_number = 1:5000) %>%
  mutate(
    strap_sample = map(strap_number, ~bootstrap_fn(weather_df))
  ) %>%
  unnest(strap_sample)

bootstrap_summary = 
  bootstrap_results %>%
  summarize(
    r_squared_mean = mean(r_squared),
    r_squared_sd = sd(r_squared),
    r_squared_ci_lower = quantile(r_squared, 0.025),
    r_squared_ci_upper = quantile(r_squared, 0.975),
    log_beta_mean = mean(log_beta_product),
    log_beta_sd = sd(log_beta_product),
    log_beta_ci_lower = quantile(log_beta_product, 0.025),
    log_beta_ci_upper = quantile(log_beta_product, 0.975)
  )

plot_r_squared = 
  ggplot(bootstrap_results, aes(x = r_squared)) +
  geom_density() +
  labs(
    title = "Bootstrap Distribution of R-squared",
    x = "R-squared",
    y = "Density"
  )

plot_log_bet
```
Problem 2 

```{r}
# Load required libraries
library(readxl)
library(dplyr)
library(broom)
library(purrr)
library(ggplot2)

homicide_data <- read_excel("homicide-data.xlsx")

# Clean and prepare the data
cleaned_data <- homicide_data %>%
  mutate(
    city_state = paste(city, state, sep = ", "),
    solved_binary = ifelse(disposition == "Closed by arrest", 1, 0),
    victim_age = as.numeric(victim_age)
  ) %>%
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
    victim_race %in% c("White", "Black"),
    !is.na(victim_age)
  )

baltimore_data <- cleaned_data %>%
  filter(city_state == "Baltimore, MD")

baltimore_glm <- glm(
  solved_binary ~ victim_age + victim_sex + victim_race,
  family = binomial(),
  data = baltimore_data
)

baltimore_tidy <- broom::tidy(baltimore_glm, exponentiate = TRUE, conf.int = TRUE)

# Extract adjusted odds ratio for male vs female
adjusted_or_baltimore <- baltimore_tidy %>%
  filter(term == "victim_sexMale") %>%
  select(estimate, conf.low, conf.high)

adjusted_or_baltimore

citywise_results <- cleaned_data %>%
  group_by(city_state) %>%
  nest() %>%
  mutate(
    glm_model = map(data, ~ glm(
      solved_binary ~ victim_age + victim_sex + victim_race,
      family = binomial(),
      data = .x
    )),
    tidy_model = map(glm_model, ~ broom::tidy(.x, exponentiate = TRUE, conf.int = TRUE))
  ) %>%
  unnest(tidy_model) %>%
  filter(term == "victim_sexMale") %>%
  select(city_state, estimate, conf.low, conf.high)

citywise_results <- citywise_results %>%
  arrange(estimate)

ggplot(citywise_results, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Adjusted Odds Ratios by City",
    x = "City",
    y = "Adjusted Odds Ratio (Male vs Female)"
  ) +
  theme_minimal()

```
The plot shows adjusted odds ratios for solving homicides comparing male to female victims across U.S. cities. Most cities have ORs close to 1, indicating no significant gender effect on case resolution. Some cities, like Albuquerque (OR > 1), show higher resolution rates for male victims, while others, like New York (OR < 1), favor female victims. Wider confidence intervals in cities like Stockton suggest greater uncertainty due to smaller sample sizes. These variations may reflect societal, cultural, or data quality differences. Further investigation into extreme cases and additional variables could provide more insight.

Problem 3
```{r}
install.packages("tidymodels")
library(readxl)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(modelr)
library(purrr)

birthweight_data <- read_excel("birthweight.xlsx")

str(birthweight_data)

birthweight_clean <- birthweight_data %>%
  mutate(
    babysex = factor(babysex, labels = c("Male", "Female")),
    frace = factor(frace),
    mrace = factor(mrace),
    malform = factor(malform),
    smoked = as.numeric(smoken), # Convert smoking variable if needed
    bwt = as.numeric(bwt) # Ensure birthweight is numeric
  ) %>%
  drop_na() # Drop rows with missing values

hypothesized_model <- lm(bwt ~ babysex + bhead + blength + fincome + smoked + wtgain, data = birthweight_clean)

summary(hypothesized_model)

birthweight_clean <- birthweight_clean %>%
  add_predictions(hypothesized_model, var = "predicted_bwt") %>%
  add_residuals(hypothesized_model, var = "residuals")


ggplot(birthweight_clean, aes(x = predicted_bwt, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Birthweight",
    y = "Residuals"
  ) +
  theme_minimal()


model_2 <- lm(bwt ~ blength + gaweeks, data = birthweight_clean)

summary(model_2)


model_3 <- lm(bwt ~ bhead * blength * babysex, data = birthweight_clean)

summary(model_3)


set.seed(123)
cv_folds <- vfold_cv(birthweight_clean, v = 5)

calculate_rmse <- function(model, data) {
  rmse <- sqrt(mean(residuals(model)^2))
  return(rmse)
}

rmse_hypothesized <- calculate_rmse(hypothesized_model, birthweight_clean)
rmse_model_2 <- calculate_rmse(model_2, birthweight_clean)
rmse_model_3 <- calculate_rmse(model_3, birthweight_clean)

data.frame(
  Model = c("Hypothesized Model", "Model 2", "Model 3"),
  RMSE = c(rmse_hypothesized, rmse_model_2, rmse_model_3)
)

comparison_results <- data.frame(
  Model = c("Hypothesized Model", "Model 2", "Model 3"),
  RMSE = c(rmse_hypothesized, rmse_model_2, rmse_model_3)
)

ggplot(comparison_results, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity", alpha = 0.7) +
  labs(
    title = "Model RMSE Comparison",
    x = "Model",
    y = "RMSE"
  ) +
  theme_minimal()

```
To analyze the factors affecting birthweight, I followed these steps:

Data Cleaning: I ensured that all relevant variables were in the appropriate format. Missing values were removed to ensure data integrity.

Hypothesized Model: I built a regression model using a hypothesized set of predictors: baby sex, head circumference, length at birth, family income, smoking during pregnancy, and maternal weight gain. These predictors were chosen based on their plausible influence on birthweight.

Residual Analysis: Using the add_predictions and add_residuals functions from the modelr package, I added fitted values and residuals to the dataset. This allowed me to evaluate the model fit by plotting residuals against fitted values.

Alternative Models for Comparison:

Model 2: A simpler model using only birth length and gestational age as predictors.
Model 3: A more complex model including head circumference, birth length, baby sex, and their interactions.
Model Evaluation: The models were compared using cross-validation and RMSE to determine predictive performance.

The first figure displays the regression results for a three-way interaction model, showing significant effects of head circumference, length, and sex on birthweight, along with their interactions. Female babies have higher average birthweights, and interactions indicate that the impact of head circumference and length varies by sex. The second figure, a residuals vs. fitted values plot, shows a generally good fit with randomly scattered residuals, although a few outliers at extreme birthweights suggest potential for further investigation. The third figure compares RMSE values for three models, highlighting the hypothesized model as the best-performing with the lowest error, followed by the interaction model, and lastly, the model with fewer predictors.